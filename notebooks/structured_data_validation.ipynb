{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2caf0275",
   "metadata": {},
   "source": [
    "# Structured Data Quality Validation\n",
    "\n",
    "This notebook validates the quality of structured data in our BDM project using Great Expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffab06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Expectations version: 1.5.0\n",
      "Setup complete ✓\n"
     ]
    }
   ],
   "source": [
    "# Install Great Expectations if needed\n",
    "# !pip install great-expectations\n",
    "\n",
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Great Expectations version:\", ge.__version__)\n",
    "print(\"Setup complete ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02368f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context type: EphemeralDataContext\n",
      "Great Expectations initialized ✓\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Great Expectations context\n",
    "context = ge.get_context(mode=\"ephemeral\")\n",
    "print(f\"Context type: {type(context).__name__}\")\n",
    "print(\"Great Expectations initialized ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd93ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_duckdb():\n",
    "    \"\"\"Load structured data from DuckDB trusted zone\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    try:\n",
    "        con = duckdb.connect(\"../bdm_project.duckdb\") \n",
    "        \n",
    "        # Show available tables\n",
    "        tables = con.execute(\"SHOW TABLES\").fetchall()\n",
    "        table_names = [table[0] for table in tables]\n",
    "        print(f\"Available tables: {table_names}\")\n",
    "        \n",
    "        table_mappings = {\n",
    "            'environmental': 'trusted_environmental_indicators',\n",
    "            'passenger': 'trusted_passenger_volume',             \n",
    "            'administrative': 'trusted_admin_boundaries'         \n",
    "        }\n",
    "        \n",
    "        for dataset_key, table_name in table_mappings.items():\n",
    "            datasets[dataset_key] = None\n",
    "            if table_name in table_names:\n",
    "                try:\n",
    "                    df = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "                    datasets[dataset_key] = df\n",
    "                    print(f\"✓ {dataset_key.title()}: {len(df)} rows from {table_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {table_name}: {e}\")\n",
    "            else:\n",
    "                print(f\"{dataset_key.title()}: Table {table_name} not found\")\n",
    "        \n",
    "        con.close()\n",
    "        return datasets\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to DuckDB: {e}\")\n",
    "        return {'environmental': None, 'passenger': None, 'administrative': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdf90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_files():\n",
    "    \"\"\"Fallback: Load data from Parquet files\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    file_paths = {\n",
    "        'environmental': [\n",
    "            \"../storage/delta/trusted/structured/environmental_indicators/\",\n",
    "            \"../storage/delta/raw/environmental_indicators/\"\n",
    "        ],\n",
    "        'passenger': [\n",
    "            \"../storage/delta/trusted/structured/passenger_volume/\",\n",
    "            \"../storage/delta/raw/passenger_volume/\"\n",
    "        ],\n",
    "        'administrative': [\n",
    "            \"../storage/delta/trusted/structured/administrative_shapefiles/\",\n",
    "            \"../storage/delta/raw/administrative_shapefiles/\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for dataset_key, paths in file_paths.items():\n",
    "        datasets[dataset_key] = None\n",
    "        \n",
    "        for path in paths:\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    # Find parquet files\n",
    "                    parquet_files = list(Path(path).glob(\"*.parquet\"))\n",
    "                    if parquet_files:\n",
    "                        # Get the most recent file\n",
    "                        latest_file = max(parquet_files, key=os.path.getmtime)\n",
    "                        df = pd.read_parquet(latest_file)\n",
    "                        datasets[dataset_key] = df\n",
    "                        print(f\"✓ {dataset_key.title()}: {len(df)} rows from {latest_file.name}\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load from {path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if datasets[dataset_key] is None:\n",
    "            print(f\"{dataset_key.title()}: No files found\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70da3749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading structured datasets...\n",
      "==================================================\n",
      "Available tables: ['trusted_admin_boundaries', 'trusted_environmental_indicators', 'trusted_passenger_volume']\n",
      "✓ Environmental: 7 rows from trusted_environmental_indicators\n",
      "✓ Passenger: 528 rows from trusted_passenger_volume\n",
      "✓ Administrative: 10 rows from trusted_admin_boundaries\n",
      "\n",
      "Data loading complete!\n",
      "Datasets available: ['environmental', 'passenger', 'administrative']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading structured datasets...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Try DuckDB first\n",
    "datasets = load_from_duckdb()\n",
    "\n",
    "# If no data found, try files\n",
    "if all(v is None for v in datasets.values()):\n",
    "    print(\"\\nNo data in DuckDB, trying files...\")\n",
    "    datasets = load_from_files()\n",
    "\n",
    "print(\"\\nData loading complete!\")\n",
    "print(f\"Datasets available: {[k for k, v in datasets.items() if v is not None]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914b0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENVIRONMENTAL DATASET:\n",
      "Rows: 7, Columns: 12\n",
      "Columns: ['year', 'gwh_raccio_elecrica', 'milloes_liros_diesel_cosumidos', 'gwh_l_diesel', 'gwh_oal', 'iesidad_eergeica_wh_u', 'iesidad_carboo_g_co2_u', 'gasos_iversioes_ambieales_euros', 'cosumo_agua_m3', 'geeracio_residuos_peligrosos_oeladas', 'porceaje_rafico_viajeros_rees_baja_emisio_acusica', 'porceaje_rafico_mercacias_rees_baja_emisio_acusica']\n",
      "Sample data:\n",
      "   year  gwh_raccio_elecrica  milloes_liros_diesel_cosumidos  gwh_l_diesel  \\\n",
      "0  2019              2460.30                           72.12         711.8   \n",
      "1  2018              2388.12                           75.20         742.8   \n",
      "\n",
      "   gwh_oal  iesidad_eergeica_wh_u  iesidad_carboo_g_co2_u  \\\n",
      "0   3172.1                   94.8                    5.54   \n",
      "1   3130.9                   94.2                   21.55   \n",
      "\n",
      "   gasos_iversioes_ambieales_euros  cosumo_agua_m3  \\\n",
      "0                             2755             903   \n",
      "1                             2538          917605   \n",
      "\n",
      "   geeracio_residuos_peligrosos_oeladas  \\\n",
      "0                                 578.0   \n",
      "1                                 948.0   \n",
      "\n",
      "   porceaje_rafico_viajeros_rees_baja_emisio_acusica  \\\n",
      "0                                                 95   \n",
      "1                                                 95   \n",
      "\n",
      "   porceaje_rafico_mercacias_rees_baja_emisio_acusica  \n",
      "0                                                 78   \n",
      "1                                                 77   \n",
      "--------------------------------------------------\n",
      "\n",
      "PASSENGER DATASET:\n",
      "Rows: 528, Columns: 6\n",
      "Columns: ['codigo_estacion', 'nombre_estacion', 'nucleo_cercanias', 'tramo_horario', 'viajeros_subidos', 'viajeros_bajados']\n",
      "Sample data:\n",
      "   codigo_estacion      nombre_estacion nucleo_cercanias  tramo_horario  \\\n",
      "0            78800  MONTCADA-BIFURCACIO        BARCELONA  15:00 - 15:30   \n",
      "1            78800  MONTCADA-BIFURCACIO        BARCELONA  15:30 - 16:00   \n",
      "\n",
      "   viajeros_subidos  viajeros_bajados  \n",
      "0                29                44  \n",
      "1                57                63  \n",
      "--------------------------------------------------\n",
      "\n",
      "ADMINISTRATIVE DATASET:\n",
      "Rows: 10, Columns: 47\n",
      "Columns: ['ID_ANNEX', 'ANNEXDESCR', 'ID_TEMA', 'TEMA_DESCR', 'ID_CONJUNT', 'CONJ_DESCR', 'ID_SUBCONJ', 'SCONJ_DESC', 'ID_ELEMENT', 'ELEM_DESCR', 'NIVELL', 'NDESCR_CA', 'NDESCR_ES', 'NDESCR_EN', 'TERME', 'DISTRICTE', 'BARRI', 'AEB', 'SEC_CENS', 'GRANBARRI', 'ZUA', 'AREA_I', 'LITERAL', 'PERIMETRE', 'AREA', 'ORD_REPRES', 'CODI_UA', 'TIPUS_UA', 'NOM', 'WEB1', 'WEB2', 'WEB3', 'DOCUMENTA', 'RANGESCALA', 'TIPUS_POL', 'GRUIX_ID', 'GRUIXDIMEN', 'ESTIL_ID', 'ESTIL_QGIS', 'VALOR1QGIS', 'VALOR2QGIS', 'COL_FARCIT', 'FCOL_DESCR', 'FHEX_COLOR', 'COL_DESCR', 'HEX_COLOR7', 'geometry_wkt']\n",
      "Sample data:\n",
      "  ID_ANNEX ANNEXDESCR ID_TEMA               TEMA_DESCR ID_CONJUNT  CONJ_DESCR  \\\n",
      "0       01   Grup - I    0104  Unitats Administratives     010412  Districtes   \n",
      "1       01   Grup - I    0104  Unitats Administratives     010412  Districtes   \n",
      "\n",
      "  ID_SUBCONJ SCONJ_DESC  ID_ELEMENT           ELEM_DESCR  ... ESTIL_ID  \\\n",
      "0   01041201  Districte  0104120101  LÃ­mit de districte  ...        0   \n",
      "1   01041201  Districte  0104120101  LÃ­mit de districte  ...        0   \n",
      "\n",
      "  ESTIL_QGIS VALOR1QGIS VALOR2QGIS COL_FARCIT FCOL_DESCR FHEX_COLOR COL_DESCR  \\\n",
      "0     SÃ²lid          0          0          1      Negre    #000000     Negre   \n",
      "1     SÃ²lid          0          0          1      Negre    #000000     Negre   \n",
      "\n",
      "  HEX_COLOR7                                       geometry_wkt  \n",
      "0    #000000  MULTIPOLYGON (((2.1482361988254643 41.37623222...  \n",
      "1    #000000  POLYGON ((2.103418130488737 41.40110207297572,...  \n",
      "\n",
      "[2 rows x 47 columns]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show basic info about each dataset\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{name.upper()} DATASET:\")\n",
    "        print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(f\"Sample data:\\n{df.head(2)}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca0b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(df, dataset_name, expectations_list):\n",
    "    \"\"\"Run Great Expectations validation on a dataset\"\"\"\n",
    "    \n",
    "    if df is None:\n",
    "        print(f\"SKIP {dataset_name}: No data available\")\n",
    "        return 0, 0\n",
    "    \n",
    "    # Setup Great Expectations objects\n",
    "    source_name = f\"{dataset_name}_source\"\n",
    "    data_source = context.data_sources.add_pandas(name=source_name)\n",
    "    data_asset = data_source.add_dataframe_asset(name=f\"{dataset_name}_data\")\n",
    "    batch_definition = data_asset.add_batch_definition_whole_dataframe(f\"{dataset_name}_batch\")\n",
    "    batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df})\n",
    "    \n",
    "    # Run validations\n",
    "    passed = 0\n",
    "    total = len(expectations_list)\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()} VALIDATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, (description, expectation) in enumerate(expectations_list, 1):\n",
    "        try:\n",
    "            result = batch.validate(expectation)\n",
    "            if result['success']:\n",
    "                print(f\"PASS {i}. {description}\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"FAIL {i}. {description}\")\n",
    "                if 'result' in result:\n",
    "                    unexpected = result['result'].get('unexpected_count', 0)\n",
    "                    total_count = result['result'].get('element_count', 0)\n",
    "                    if total_count > 0:\n",
    "                        print(f\"     {unexpected}/{total_count} values failed\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR {i}. {description}\")\n",
    "    \n",
    "    # Summary\n",
    "    percentage = (passed / total * 100) if total > 0 else 0\n",
    "    print(f\"\\nSUMMARY: {passed}/{total} tests passed ({percentage:.0f}%)\")\n",
    "    \n",
    "    return passed, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d04c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENVIRONMENTAL INDICATORS VALIDATION:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 1668.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 1. Year not null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 2324.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 2. Year range 2000-2030\n",
      "\n",
      "SUMMARY: 2/2 tests passed (100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "environmental_expectations = [\n",
    "    (\"Year not null\", ge.expectations.ExpectColumnValuesToNotBeNull(column=\"year\")),\n",
    "    (\"Year range 2000-2030\", ge.expectations.ExpectColumnValuesToBeBetween(column=\"year\", min_value=2000, max_value=2030))\n",
    "]\n",
    "\n",
    "# Run validation\n",
    "env_passed, env_total = validate_dataset(datasets['environmental'], 'Environmental Indicators', environmental_expectations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6833cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PASSENGER VOLUME VALIDATION:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 2154.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 1. Station code not null\n",
      "\n",
      "SUMMARY: 1/1 tests passed (100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_passenger_expectations(df):\n",
    "    if df is None:\n",
    "        return []\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    expectations = []\n",
    "    \n",
    "    # Station validation\n",
    "    station_cols = [col for col in columns if any(keyword in col.lower() for keyword in ['station', 'estacion', 'codigo'])]\n",
    "    if station_cols:\n",
    "        expectations.append((\"Station code not null\", ge.expectations.ExpectColumnValuesToNotBeNull(column=station_cols[0])))\n",
    "    \n",
    "    # Passenger count validation\n",
    "    count_cols = [col for col in columns if any(keyword in col.lower() for keyword in ['subidas', 'bajadas', 'passengers'])]\n",
    "    for col in count_cols:\n",
    "        expectations.append((f\"{col} non-negative\", ge.expectations.ExpectColumnValuesToBeBetween(column=col, min_value=0, max_value=50000)))\n",
    "    \n",
    "    return expectations\n",
    "\n",
    "# Run validation\n",
    "passenger_expectations = get_passenger_expectations(datasets['passenger'])\n",
    "pass_passed, pass_total = validate_dataset(datasets['passenger'], 'Passenger Volume', passenger_expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3d5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ADMINISTRATIVE BOUNDARIES VALIDATION:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 2034.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 1. Geometry WKT not null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 10/10 [00:00<00:00, 2167.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 2. Geometry WKT is string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics: 100%|██████████| 8/8 [00:00<00:00, 1618.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS 3. Name not null\n",
      "\n",
      "SUMMARY: 3/3 tests passed (100%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get admin expectations based on available columns\n",
    "def get_admin_expectations(df):\n",
    "    if df is None:\n",
    "        return []\n",
    "    \n",
    "    columns = df.columns.tolist()\n",
    "    expectations = []\n",
    "    \n",
    "    if 'geometry_wkt' in columns:\n",
    "        expectations.extend([\n",
    "            (\"Geometry WKT not null\", ge.expectations.ExpectColumnValuesToNotBeNull(column=\"geometry_wkt\")),\n",
    "            (\"Geometry WKT is string\", ge.expectations.ExpectColumnValuesToBeOfType(column=\"geometry_wkt\", type_=\"str\"))\n",
    "        ])\n",
    "    \n",
    "    # Name validation\n",
    "    name_cols = [col for col in columns if any(keyword in col.lower() for keyword in ['name', 'nom', 'district'])]\n",
    "    if name_cols:\n",
    "        expectations.append((\"Name not null\", ge.expectations.ExpectColumnValuesToNotBeNull(column=name_cols[0])))\n",
    "    \n",
    "    return expectations\n",
    "\n",
    "# Run validation\n",
    "admin_expectations = get_admin_expectations(datasets['administrative'])\n",
    "admin_passed, admin_total = validate_dataset(datasets['administrative'], 'Administrative Boundaries', admin_expectations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73069cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STRUCTURED DATA QUALITY SUMMARY\n",
      "==================================================\n",
      "OVERALL: 6/6 tests passed (100%)\n",
      "Environmental: 2/2 (100%)\n",
      "Passenger: 1/1 (100%)\n",
      "Administrative: 3/3 (100%)\n",
      "\n",
      "STATUS: Excellent data quality\n",
      "==================================================\n",
      "Validation Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STRUCTURED DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate totals\n",
    "total_passed = env_passed + pass_passed + admin_passed\n",
    "total_tests = env_total + pass_total + admin_total\n",
    "\n",
    "if total_tests > 0:\n",
    "    overall_percentage = (total_passed / total_tests * 100)\n",
    "    \n",
    "    print(f\"OVERALL: {total_passed}/{total_tests} tests passed ({overall_percentage:.0f}%)\")\n",
    "    \n",
    "    # Individual summaries\n",
    "    if env_total > 0:\n",
    "        print(f\"Environmental: {env_passed}/{env_total} ({env_passed/env_total*100:.0f}%)\")\n",
    "    if pass_total > 0:\n",
    "        print(f\"Passenger: {pass_passed}/{pass_total} ({pass_passed/pass_total*100:.0f}%)\")\n",
    "    if admin_total > 0:\n",
    "        print(f\"Administrative: {admin_passed}/{admin_total} ({admin_passed/admin_total*100:.0f}%)\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    if overall_percentage >= 90:\n",
    "        print(\"\\nSTATUS: Excellent data quality\")\n",
    "    elif overall_percentage >= 75:\n",
    "        print(\"\\nSTATUS: Good data quality\")\n",
    "    elif overall_percentage >= 50:\n",
    "        print(\"\\nSTATUS: Acceptable data quality\")\n",
    "    else:\n",
    "        print(\"\\nSTATUS: Poor data quality\")\n",
    "else:\n",
    "    print(\"No datasets available for validation\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Validation Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
