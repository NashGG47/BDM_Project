from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS
from influxdb_client.domain.bucket_retention_rules import BucketRetentionRules
from datetime import datetime
import pandas as pd


url = "http://localhost:8086"
token = "token1"
org = "upa"
new_bucket = "gencat_incident_clusters"

client = InfluxDBClient(url=url, token=token, org=org)
query_api = client.query_api()


query = '''
from(bucket: "gencat_incidents")
  |> range(start: 0)
  |> filter(fn: (r) => r._measurement == "traffic_incidents")
  |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
  |> keep(columns: ["road", "cause", "lat", "lon", "incident_id"])
'''

tables = query_api.query_data_frame(org=org, query=query)
df = pd.concat(tables) if isinstance(tables, list) else tables


grouped = df.groupby(["road", "cause"]).agg({
    "incident_id": "count",
    "lat": "mean",
    "lon": "mean"
}).reset_index()

grouped.rename(columns={"incident_id": "incident_count"}, inplace=True)

retention_rules = BucketRetentionRules(type="expire", every_seconds=0)
buckets_api = client.buckets_api()
bucket = buckets_api.create_bucket(bucket_name=new_bucket, org=org, retention_rules=[retention_rules])
print(f"Bucket '{new_bucket}' created successfully.")


write_api = client.write_api(write_options=SYNCHRONOUS)

for _, row in grouped.iterrows():
    point = (
        Point("incident_clusters")
        .tag("road", row["road"])
        .tag("cause", row["cause"])
        .field("incident_count", int(row["incident_count"]))
        .field("lat", float(row["lat"]))
        .field("lon", float(row["lon"]))
        .time(datetime.utcnow(), WritePrecision.NS)
    )
    write_api.write(bucket=new_bucket, org=org, record=point)

print("Clustered data written to InfluxDB.")
client.close()
