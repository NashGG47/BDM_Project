from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import subprocess
import logging


def exploit_map_job():
    logging.info("Running exploitation_zone/showcase_emissions_shapefile.py...")

    try:
        result = subprocess.run(
            ["python", "exploitation_zone/showcase_emissions_shapefile.py"],
            capture_output=True,
            text=True,
            check=True
        )
        logging.info("Script completed successfully.")
        logging.info(f"stdout: {result.stdout}")
        logging.info(f"stderr: {result.stderr}")
    except subprocess.CalledProcessError as e:
        logging.error("Script failed!")
        logging.error(f"Return code: {e.returncode}")
        logging.error(f"stdout: {e.stdout}")
        logging.error(f"stderr: {e.stderr}")
        raise

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 3, 15),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "exploit_map_dag",
    description="Process exploit data",
    schedule=timedelta(minutes=30),  # or timedelta(hours=1), depending on your use case
    catchup=False,
    default_args=default_args,
)

run_exploit_task = PythonOperator(
    task_id="map_exploit_job",
    python_callable=exploit_map_job,
    dag=dag,
)

run_exploit_task